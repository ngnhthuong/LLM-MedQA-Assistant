models:
  - type: openai-chat         # NeMo treats your vLLM endpoint as OpenAI-chat compatible
    name: medqa-llm
    engine: ${LLM_MODEL_ID}   # reuse your env var
    params:
      temperature: 0.2
    # base_url: points to your external vLLM / KServe gateway
    # If your vLLM exposes /v1/chat/completions:
    api_base: ${KSERVE_BASE_URL}

# Which flows to run by default
rails:
  input:
    flows:
      - medical_chat
  output:
    flows:
      - medical_chat

# Optional: tell NeMo where the CoLang flows live
colang_paths:
  - flows