# metrics_llm.py
from prometheus_client import Counter, Histogram

LLM_REQUESTS_TOTAL = Counter(
    "llm_requests_total",
    "Total number of LLM inference requests",
    ["model", "status"],
)

LLM_INFERENCE_LATENCY_SECONDS = Histogram(
    "llm_inference_latency_seconds",
    "Latency of LLM inference calls",
    ["model"],
    buckets=(0.2, 0.5, 1, 2, 5, 10, 20, 40, 60),
)

LLM_PROMPT_TOKENS_TOTAL = Counter(
    "llm_prompt_tokens_total",
    "Total prompt tokens sent to the LLM",
    ["model"],
)

LLM_COMPLETION_TOKENS_TOTAL = Counter(
    "llm_completion_tokens_total",
    "Total completion tokens generated by the LLM",
    ["model"],
)
